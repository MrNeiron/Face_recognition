{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, Lambda, MaxPooling2D, Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras.engine.topology import Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Prepare_data import Get_data\n",
    "from utils import to_file_params, TimeControll, data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from os.path import exists\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN\n",
    "TYPE = 0\n",
    "\n",
    "#OTHERS\n",
    "DESCRIPTION = \"Testing generator\"\n",
    "PRINT_LOGS = False\n",
    "SHOW_FIGURES = True\n",
    "\n",
    "#IMAGES\n",
    "RESOLUTION = (96,96)\n",
    "GRAYSCALE = True\n",
    "INPUT_SHAPE = (RESOLUTION[0], RESOLUTION[1], 1 if GRAYSCALE else 3)\n",
    "NUM_EXAMPLES = 15 if TYPE != 0 else 15\n",
    "START_EXAMPLES = 0 if TYPE != 0 else 0\n",
    "\n",
    "#FOLDERS\n",
    "NUM_FOLDERS = 40 if TYPE != 0 else 5\n",
    "START_FOLDER = 0 if TYPE != 0 else 0\n",
    "BATCH_SIZE_FOLDER = 2 if TYPE != 0 else 2\n",
    "\n",
    "#FOLDERS VALIDATION\n",
    "NUM_FOLDERS_VAL = 20 if TYPE != 0 else 2\n",
    "START_FOLDER_VAL = 900 if TYPE != 0 else 900\n",
    "BATCH_SIZE_FOLDER_VAL = 2 if TYPE != 0 else 2\n",
    "\n",
    "\n",
    "#TRAIN ITTERATION\n",
    "#BATCH_SIZE = 80 if TYPE != 0 else 50\n",
    "EPOCHS = 20 if TYPE != 0 else 3\n",
    "\n",
    "#TRAIN\n",
    "LEARNING_RATE = 0.001\n",
    "L2 = 0.0007\n",
    "\n",
    "#DIRECTIONS\n",
    "NAME_HISTORY = f\"history/FaceModel_whaleType/train/history{TYPE}.txt\" if TYPE != 0 else f\"history/FaceModel_whaleType/train/history_train_test{TYPE}.txt\"\n",
    "NAME_JSON_HISTORY = f\"history/FaceModel_whaleType/JSON/history{TYPE}.json\" if TYPE != 0 else f\"history/FaceModel_whaleType/JSON/history_test{TYPE}.json\"\n",
    "\n",
    "NAME_FIGURE = f\"figures/FaceModel_whaleType/train/{TYPE}/\"\n",
    "\n",
    "SAVE_MODEL_DIR = \"models/FaceModel_whaleType\"\n",
    "SAVE_EACH_MODEL_DIR = f\"{SAVE_MODEL_DIR}/{TYPE}\"\n",
    "\n",
    "SAVE_MODEL_NAME = f\"{SAVE_MODEL_DIR}/Model_{TYPE}.h5\" if TYPE != 0 else f\"{SAVE_MODEL_DIR}/Model_test_{TYPE}.h5\" \n",
    "SAVE_EACH_MODEL_NAME = (f\"{SAVE_EACH_MODEL_DIR}/Model_\" + \"{epoch:02d}-{val_loss:.2f}.h5\") if TYPE != 0 else (f\"{SAVE_EACH_MODEL_DIR}/Model_test_\" + \"{epoch:02d}-{val_loss:.2f}.h5\")\n",
    "\n",
    "DATASET_P = DATASET_N = \"../Datasets/Faces_dataset/Faces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EXAMPLES = NUM_EXAMPLES*NUM_FOLDERS*2\n",
    "TEST_EXAMPLES = NUM_EXAMPLES*NUM_FOLDERS_VAL*2\n",
    "ALL_EXAMPLES = TRAIN_EXAMPLES+TEST_EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = data_generator(DATASET_P,\n",
    "                          DATASET_N,\n",
    "                          resolution = RESOLUTION,\n",
    "                          grayscale = GRAYSCALE,\n",
    "                          num_examples = NUM_EXAMPLES,\n",
    "                          start_examples = START_EXAMPLES,\n",
    "                          num_folders = NUM_FOLDERS,\n",
    "                          start_folder = START_FOLDER,\n",
    "                          batch_size_folder = BATCH_SIZE_FOLDER,\n",
    "                          input_shape = INPUT_SHAPE,\n",
    "                          print_logs = PRINT_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valGen = data_generator(DATASET_P,\n",
    "                        DATASET_N,\n",
    "                        resolution = RESOLUTION,\n",
    "                        grayscale = GRAYSCALE,\n",
    "                        num_examples = NUM_EXAMPLES,\n",
    "                        start_examples = START_EXAMPLES,\n",
    "                        num_folders = NUM_FOLDERS_VAL,\n",
    "                        start_folder = START_FOLDER_VAL,\n",
    "                        batch_size_folder = BATCH_SIZE_FOLDER_VAL,\n",
    "                        input_shape = INPUT_SHAPE,\n",
    "                        print_logs = PRINT_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subblock(x, filter, **kwargs):\n",
    "    x = BatchNormalization()(x)\n",
    "    y = x\n",
    "    y = Conv2D(filter, (1, 1), activation='relu', **kwargs)(y) # Reduce the number of features to 'filter'\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(filter, (3, 3), activation='relu', **kwargs)(y) # Extend the feature field\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y) # no activation # Restore the number of original features\n",
    "    y = Add()([x,y]) # Add the bypass connection\n",
    "    y = Activation('relu')(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(img_shape, lr, l2, activation='sigmoid'):\n",
    "\n",
    "    ##############\n",
    "    # BRANCH MODEL\n",
    "    ##############\n",
    "    regul  = regularizers.l2(l2)\n",
    "    optim  = Adam(lr=lr)\n",
    "    kwargs = {'padding':'same', 'kernel_regularizer':regul}\n",
    "\n",
    "    inp = Input(shape=img_shape) # 384x384x1\n",
    "    x   = Conv2D(64, (9,9), strides=2, activation='relu', **kwargs)(inp)\n",
    "\n",
    "    x   = MaxPooling2D((2, 2), strides=(2, 2))(x) # 96x96x64\n",
    "    for _ in range(1):\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(64, (3,3), activation='relu', **kwargs)(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 48x48x64\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (1,1), activation='relu', **kwargs)(x) # 48x48x128\n",
    "    for _ in range(2): x = subblock(x, 64, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 24x24x128\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (1,1), activation='relu', **kwargs)(x) # 24x24x256\n",
    "    for _ in range(2): x = subblock(x, 64, **kwargs)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 12x12x256\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(384, (1,1), activation='relu', **kwargs)(x) # 12x12x384\n",
    "    for _ in range(2): x = subblock(x, 96, **kwargs)\n",
    "\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 6x6x384\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (1,1), activation='relu', **kwargs)(x) # 6x6x512\n",
    "    for _ in range(2): x = subblock(x, 128, **kwargs)\n",
    "    \n",
    "    x             = GlobalMaxPooling2D()(x) # 512\n",
    "    branch_model  = Model(inp, x)\n",
    "    \n",
    "    ############\n",
    "    # HEAD MODEL\n",
    "    ############\n",
    "    mid        = 32\n",
    "    xa_inp     = Input(shape=branch_model.output_shape[1:])\n",
    "    xb_inp     = Input(shape=branch_model.output_shape[1:])\n",
    "    x1         = Lambda(lambda x : x[0]*x[1])([xa_inp, xb_inp])\n",
    "    x2         = Lambda(lambda x : x[0] + x[1])([xa_inp, xb_inp])\n",
    "    x3         = Lambda(lambda x : K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n",
    "    x4         = Lambda(lambda x : K.square(x))(x3)\n",
    "    x          = Concatenate()([x1, x2, x3, x4])\n",
    "\n",
    "    x          = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n",
    "\n",
    "    # Per feature NN with shared weight is implemented using CONV2D with appropriate stride.\n",
    "    x          = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n",
    "    x          = Reshape((branch_model.output_shape[1], mid, 1))(x)\n",
    "    x          = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n",
    "    x          = Flatten(name='flatten')(x)\n",
    "    \n",
    "    # Weighted sum implemented as a Dense layer.\n",
    "    x          = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n",
    "    head_model = Model([xa_inp, xb_inp], x, name='head')\n",
    "\n",
    "    ########################\n",
    "    # SIAMESE NEURAL NETWORK\n",
    "    ########################\n",
    "    # Complete model is constructed by calling the branch model on each input image,\n",
    "    # and then the head model on the resulting 512-vectors.\n",
    "    img_a      = Input(shape=img_shape)\n",
    "    img_b      = Input(shape=img_shape)\n",
    "    xa         = branch_model(img_a)\n",
    "    xb         = branch_model(img_b)\n",
    "    x          = head_model([xa, xb])\n",
    "    model      = Model([img_a, img_b], x)\n",
    "    model.compile(optim, loss='binary_crossentropy', metrics=['binary_crossentropy', 'acc'])\n",
    "    return model, branch_model, head_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceModel, _,_ = build_model(INPUT_SHAPE, lr = LEARNING_RATE, l2 = L2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file_params(NAME_HISTORY, [f\"input_shape = {INPUT_SHAPE}\\nnum_examples = {NUM_EXAMPLES}\\nnum_folders = {NUM_FOLDERS}\\nstart_folder = {START_FOLDER}\\nbatch_size_folder = {BATCH_SIZE_FOLDER}\\nnum_folders_val = {NUM_FOLDERS_VAL}\\nstart_folder_val = {START_FOLDER_VAL}\\nbatch_size_folder_val = {BATCH_SIZE_FOLDER_VAL}\\nepochs = {EPOCHS}\\nlearning_rate = {LEARNING_RATE}\\nl2 = {L2}\\nname_history = {NAME_HISTORY}\\nsave_model_name = {SAVE_MODEL_NAME}\\ndataset_positive = {DATASET_P}\\ndataset_negative = {DATASET_N}\\ndescription = {DESCRIPTION}\\n\"], with_lines = False)\n",
    "\n",
    "to_file_params(NAME_HISTORY, [f\"all_examples = {ALL_EXAMPLES}\\ntrain_examples = {TRAIN_EXAMPLES}\\ntest_examples = {TEST_EXAMPLES}\\n\"], with_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(SAVE_EACH_MODEL_DIR):\n",
    "    makedirs(SAVE_EACH_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = ModelCheckpoint(SAVE_EACH_MODEL_NAME, \n",
    "                            monitor=[\"val_loss\"], \n",
    "                            mode=\"min\",\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = [callbacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_time = TimeControll()\n",
    "time_start = my_time.get_start_time()\n",
    "to_file_params(NAME_HISTORY, [f\"\\tStart time = {time_start[0]}:{time_start[1]}\"], with_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2/2 [==============================] - 32s 16s/step - loss: 3.4469 - binary_crossentropy: 0.6835 - acc: 0.6000 - val_loss: 3.5236 - val_binary_crossentropy: 0.7762 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: saving model to models/FaceModel_whaleType/0/Model_test_01-3.52.h5\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 1s 479ms/step - loss: 3.4237 - binary_crossentropy: 0.6822 - acc: 0.5083 - val_loss: 3.3989 - val_binary_crossentropy: 0.6750 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00002: saving model to models/FaceModel_whaleType/0/Model_test_02-3.40.h5\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 3.4127 - binary_crossentropy: 0.6946 - acc: 0.5250 - val_loss: 3.3706 - val_binary_crossentropy: 0.6698 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00003: saving model to models/FaceModel_whaleType/0/Model_test_03-3.37.h5\n"
     ]
    }
   ],
   "source": [
    "hist = FaceModel.fit_generator(trainGen, \n",
    "                               epochs = EPOCHS, \n",
    "                               steps_per_epoch = NUM_FOLDERS//BATCH_SIZE_FOLDER,\n",
    "                               validation_data = valGen,\n",
    "                               validation_steps = NUM_FOLDERS_VAL//BATCH_SIZE_FOLDER_VAL,\n",
    "                               callbacks = callback_list,\n",
    "                               verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_time.set_end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FaceModel.save(SAVE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = my_time.get_end_time()\n",
    "time_spend = my_time.get_spend_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file_params(NAME_HISTORY,[f\"\\tEnd time = {time_end[0]}:{time_end[1]}\\n\\tSpend time = {time_spend[0]}:{time_spend[1]}:{time_spend[2]}\"], with_lines = False)\n",
    "\n",
    "to_file_params(NAME_HISTORY,[f\"\\tAccuracy: {hist.history['acc'][-1]}, Val Accuracy: {hist.history['val_acc'][-1]}, Loss: {hist.history['loss'][-1]}, Val Loss: {hist.history['val_loss'][-1]}\"], with_lines = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [f\"Epoch {i+1}/{EPOCHS}\\n\\t\\t loss: {loss}, acc: {acc}, val_loss: {val_loss}, val_acc: {val_acc}\" for i,(loss,acc,val_loss,val_acc) in enumerate(zip(hist.history[\"loss\"],hist.history[\"acc\"],hist.history[\"val_loss\"],hist.history[\"val_acc\"]))]\n",
    "\n",
    "to_file_params(NAME_HISTORY, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing history in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(history_dict, open(NAME_JSON_HISTORY, 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5033333321412404,\n",
       " 0.5344444493452708,\n",
       " 0.5688888887564342,\n",
       " 0.5133333325386047,\n",
       " 0.5666666666666667,\n",
       " 0.6199999968210856,\n",
       " 0.6755555510520935,\n",
       " 0.746666673819224,\n",
       " 0.751111110051473,\n",
       " 0.7522222240765889,\n",
       " 0.7955555597941081,\n",
       " 0.8022222240765889,\n",
       " 0.8544444441795349,\n",
       " 0.896666657924652,\n",
       " 0.950000003973643,\n",
       " 0.9711111028989156,\n",
       " 0.9699999968210856,\n",
       " 0.9355555534362793,\n",
       " 0.9377777695655822,\n",
       " 0.9377777695655822]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#history_dict[\"acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(NAME_JSON_HISTORY, 'r') as fl:\n",
    "    json_file = json.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5033333321412404,\n",
       " 0.5344444493452708,\n",
       " 0.5688888887564342,\n",
       " 0.5133333325386047,\n",
       " 0.5666666666666667,\n",
       " 0.6199999968210856,\n",
       " 0.6755555510520935,\n",
       " 0.746666673819224,\n",
       " 0.751111110051473,\n",
       " 0.7522222240765889,\n",
       " 0.7955555597941081,\n",
       " 0.8022222240765889,\n",
       " 0.8544444441795349,\n",
       " 0.896666657924652,\n",
       " 0.950000003973643,\n",
       " 0.9711111028989156,\n",
       " 0.9699999968210856,\n",
       " 0.9355555534362793,\n",
       " 0.9377777695655822,\n",
       " 0.9377777695655822]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json_file[\"acc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_process(history, metrics):\n",
    "    metrics_list = {m:history[m] for m in metrics}\n",
    "    epochs = range(len(metrics_list[metrics[0]]))\n",
    "    \n",
    "    if not exists(NAME_FIGURE):\n",
    "        makedirs(NAME_FIGURE)\n",
    "    \n",
    "    for metric,values in metrics_list.items():\n",
    "        plt.plot(epochs, values)\n",
    "        plt.title(f\"Type: {TYPE} - {metric}\")\n",
    "\n",
    "\n",
    "        if TYPE != 0:\n",
    "            plt.savefig(NAME_FIGURE + f'{metric}-{values[-1]:.2f}.jpg', quality = 100, dpi = 150)\n",
    "        else:\n",
    "            plt.savefig(NAME_FIGURE + f'test_{metric}-{values[-1]:.2f}.jpg', quality = 100, dpi = 150)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        #plt.savefig(\"history/test_accuracy.jpg\", bbox_inches = \"tight\", quality = 100, dpi = 150)#save_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'monitor_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-6d66cd00193a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mSHOW_FIGURES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitor_process\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmonitor_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"val_acc\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNAME_FIGURE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'monitor_process'"
     ]
    }
   ],
   "source": [
    "if SHOW_FIGURES:\n",
    "    from utils import monitor_process\n",
    "    \n",
    "    monitor_process(hist.history, [\"acc\",\"val_acc\",\"loss\",\"val_loss\"], NAME_FIGURE, TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = FaceModel.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_history/json_model.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
