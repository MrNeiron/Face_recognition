{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, Flatten, merge, Dense, Flatten,MaxPooling2D,AveragePooling2D, ZeroPadding2D, Activation, concatenate, Subtract\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "#import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "#import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from fr_utils import *\n",
    "#from inception_blocks_orig import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self,Xtrain,Xval):\n",
    "        self.Xval = Xval\n",
    "        self.Xtrain = Xtrain\n",
    "        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape\n",
    "        self.n_val,self.n_ex_val,_,_ = Xval.shape\n",
    "\n",
    "    def get_batch(self,n):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        categories = numpy.random.choice(self.n_classes,size=(n,),replace=False)\n",
    "        pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)]\n",
    "        targets=np.zeros((n,))\n",
    "        targets[n//2:] = 1\n",
    "        for i in range(n):\n",
    "            category = categories[i]\n",
    "            idx_1 = numpy.random.randint(0,self.n_examples)\n",
    "            pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1)\n",
    "            idx_2 = numpy.random.randint(0,self.n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            category_2 = category if i >= n//2 else (category + numpy.random.randint(1,self.n_classes)) % self.n_classes\n",
    "            pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1)\n",
    "        return pairs, targets\n",
    "\n",
    "    def make_oneshot_task(self,N):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        categories = numpy.random.choice(self.n_val,size=(N,),replace=False)\n",
    "        indices = numpy.random.randint(0,self.n_ex_val,size=(N,))\n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = numpy.random.choice(self.n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
    "        support_set = self.Xval[categories,indices,:,:]\n",
    "        support_set[0,:,:] = self.Xval[true_category,ex2]\n",
    "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
    "        pairs = [test_image,support_set]\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        return pairs, targets\n",
    "\n",
    "    def test_oneshot(self,model,N,k,verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        pass\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == 0:\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape, name = None):\n",
    "    values = np.random.normal(loc = 0, scale = 1e-2, size = shape)\n",
    "    return K.variable(values, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_init(shape, name = None):\n",
    "    values = np.random.normal(loc = 0.5, scale = 1e-2, size = shape)\n",
    "    return K.variable(values, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input1, X_output1 = faceRecoModel(input_shape = (3, 96, 96))\n",
    "X_input2, X_output2 = faceRecoModel(input_shape = (3, 96 ,96), add = '_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "#both = merge([model1,model2], mode = L1_distance, output_shape = lambda x: x[0])\n",
    "sub = Subtract()([X_output1,X_output2])\n",
    "prediction = Dense(1, activation = \"sigmoid\", bias_initializer=b_init)(sub)\n",
    "siamese_net = Model(input=[X_input1, X_input2], output = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.00006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.compile(loss = \"binary_crossentropy\", optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7486689"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[take_images(\"images/myImages/ivan.jpg\"), take_images(\"images/myImages/ivan2.jpg\")],\n",
    "          [take_images(\"images/myImages/ivan.jpg\"), take_images(\"images/myImages/pasha.jpg\")],\n",
    "          [take_images(\"images/myImages/pasha.jpg\"), take_images(\"images/myImages/sasha.jpg\")],\n",
    "          [take_images(\"images/myImages/ivan.jpg\"), take_images(\"images/myImages/ivan3.jpg\")],\n",
    "          [take_images(\"images/myImages/danil.jpg\"), take_images(\"images/myImages/slava.jpg\")],\n",
    "          [take_images(\"images/myImages/zjenya.jpg\"), take_images(\"images/myImages/alinka.jpg\")],\n",
    "          [take_images(\"images/myImages/ivan2.jpg\"), take_images(\"images/myImages/ivan3.jpg\")]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [[1],[0],[0],[1],[0],[0],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smth1 = [2,3,4,5, 2, 3, 3]\n",
    "smth = np.random.choice(smth1, 2, replace = False)\n",
    "smth2 = [smth1[i] for i in smth]\n",
    "print(smth)\n",
    "print(smth2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "    nums = np.random.choice(len(x), batch_size, replace = False)\n",
    "    print(nums)\n",
    "    x_batch = [x[i] for i in nums]\n",
    "    y_batch = [y[i] for i in nums]\n",
    "    \n",
    "    return (x_batch,y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 3]\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x_new, y_new = get_batch(x_train, y_train, 3)\n",
    "print(len(x_train[0]))\n",
    "print(len(x_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, size):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    categories = numpy.random.choice(self.n_classes,size=(n,),replace=False)\n",
    "    pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)]\n",
    "    targets=np.zeros((n,))\n",
    "    targets[n//2:] = 1\n",
    "    for i in range(n):\n",
    "        category = categories[i]\n",
    "        idx_1 = numpy.random.randint(0,self.n_examples)\n",
    "        pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1)\n",
    "        idx_2 = numpy.random.randint(0,self.n_examples)\n",
    "        #pick images of same class for 1st half, different for 2nd\n",
    "        category_2 = category if i >= n//2 else (category + numpy.random.randint(1,self.n_classes)) % self.n_classes\n",
    "        pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1)\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-60c77550334a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSiamese_Loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-bd937c700fdd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, Xtrain, Xval)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_ex_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "loader = Siamese_Loader(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_every = 7000\n",
    "loss_every = 300\n",
    "batch_size = 32\n",
    "N_way = 20\n",
    "n_val = 550\n",
    "load_weights_from_FaceNet(siamese_net)\n",
    "#siamese_net.load_weights(\"E:/weights/\")\n",
    "best = 76.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 3 arrays: [array([[[[[0.98823529, 0.99215686, 0.98823529, 0.98823529, 0.98823529,\n           0.98823529, 0.99215686, 0.98823529, 0.99215686, 0.99215686,\n           0.98823529, 0.98823529, 0.98823529, 0.98431373...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-352958903860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#loader.get_batch(batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"itteration {}, training_loss: {}, \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1207\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1209\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 3 arrays: [array([[[[[0.98823529, 0.99215686, 0.98823529, 0.98823529, 0.98823529,\n           0.98823529, 0.99215686, 0.98823529, 0.99215686, 0.99215686,\n           0.98823529, 0.98823529, 0.98823529, 0.98431373..."
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "\n",
    "for i in range(10):\n",
    "    (inputs,targets) = get_batch(x_train,y_train,3)#loader.get_batch(batch_size)\n",
    "    loss = siamese_net.train_on_batch(inputs, targets)\n",
    "    print(\"itteration {}, training_loss: {}, \".format(i, loss))\n",
    "    test = siamese_net.predict_on_batch(inputs)\n",
    "    print(\"test {}: {}\".format(i,test))\n",
    "'''    if (i % evaluate_every == 0):\n",
    "        val_acc = loader.test_onehot(siamese_net, N_way, n_val, verbos = True)\n",
    "        if (val_acc >= best):\n",
    "            print(\"saving\")\n",
    "            siamese_net.save(\"PATH\")\n",
    "            best = val_acc'''\n",
    "            \n",
    "        #if (i % loss_every == 0):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6724873]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.predict_on_batch([take_images(\"images/myImages/ivan.jpg\"),take_images(\"images/myImages/ivan2.jpg\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
